{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JgQNzc5O1MQh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BFQd2X0M1Uc7"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    imshow = cv2_imshow\n",
        "else:\n",
        "\n",
        "    def imshow(img):\n",
        "        cv2.imshow(\"ImageWindow\", img)\n",
        "        cv2.waitKey()\n",
        "\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1BspL-71RXU"
      },
      "outputs": [],
      "source": [
        "img1  = cv2.imread('img1.jpg')\n",
        "img2  = cv2.imread('img2.jpg')\n",
        "img1 = cv2.resize(img1, None, fx=0.2, fy=0.2)\n",
        "img2 = cv2.resize(img2, None, fx=0.2, fy=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qy8AJuTV2Kx6"
      },
      "outputs": [],
      "source": [
        "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_corners(image):\n",
        "    image = image.copy()\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    blurred_image = cv2.blur(gray, kernel_size)\n",
        "\n",
        "    sobel_x = cv2.Sobel(blurred_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(blurred_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "    magnitude = np.uint8(magnitude)\n",
        "\n",
        "    _, thresh = cv2.threshold(magnitude, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    corner_points = []\n",
        "\n",
        "    epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "    approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "    if len(approx) == 4:\n",
        "        corner_points.extend(approx)\n",
        "\n",
        "    corner_points = np.array(corner_points)\n",
        "    return corner_points\n",
        "\n",
        "    for corner in corner_points:\n",
        "        x, y = corner.ravel()\n",
        "        cv2.circle(image, (x, y), 10, 255, -1)\n",
        "\n",
        "    # cv2.imshow('Corners Detected', image)\n",
        "    # cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()\n",
        "        \n",
        "    return corner_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_corner_points(corners):\n",
        "    corners = [(corner[0][0], corner[0][1]) for corner in corners]\n",
        "    top_r, top_l, bottom_l, bottom_r = corners[0], corners[1], corners[2], corners[3]\n",
        "    return (top_l, top_r, bottom_r, bottom_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perspective_transform(image, corners):\n",
        "    ordered_corners = order_corner_points(corners)\n",
        "    top_l, top_r, bottom_r, bottom_l = ordered_corners\n",
        "\n",
        "    width_A = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n",
        "    width_B = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n",
        "    width = max(int(width_A), int(width_B))\n",
        "\n",
        "    height_A = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n",
        "    height_B = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n",
        "    height = max(int(height_A), int(height_B))\n",
        "\n",
        "    dimensions = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], \n",
        "                    [0, height - 1]], dtype = \"float32\")\n",
        "\n",
        "    ordered_corners = np.array(ordered_corners, dtype=\"float32\")\n",
        "\n",
        "    matrix = cv2.getPerspectiveTransform(ordered_corners, dimensions)\n",
        "\n",
        "    return cv2.warpPerspective(image, matrix, (width, height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "image = cv2.imread('img4.jpg')\n",
        "\n",
        "corners = detect_corners(image)\n",
        "print(len(corners))\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "transformed_image = perspective_transform(image, corners)\n",
        "cv2.imshow('Transformed Image', transformed_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'detect_objects' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9516/1367275847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Perform object detection on the current frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mdetected_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Draw rectangles and labels on the frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'detect_objects' is not defined"
          ]
        }
      ],
      "source": [
        "video_path = 'vid1.mp4'  # Replace with the path to your video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Couldn't open video.\")\n",
        "    exit()\n",
        "\n",
        "# Get video information\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Process the video\n",
        "frame_number = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Process every 5 frames\n",
        "    if frame_number % 5 == 0:\n",
        "        # Perform object detection on the current frame\n",
        "        detected_objects = detect_objects(frame)\n",
        "\n",
        "        # Draw rectangles and labels on the frame\n",
        "        for obj in detected_objects:\n",
        "            x, y, width, height, obj_class = obj\n",
        "            color = (0, 255, 0)  # Example color, you can customize based on class\n",
        "            cv2.rectangle(frame, (x, y), (x + width, y + height), color, 2)\n",
        "            label = f'Class {obj_class}'\n",
        "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow('Object Detection', frame)\n",
        "        cv2.waitKey(0)  # You can adjust the wait key or use cv2.waitKey(1) for a delay\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
        "        break\n",
        "\n",
        "# Release the video capture and close the window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntzhvoMbmiKk"
      },
      "source": [
        "## Detecting objects on a photo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N-_8QizEJuLH"
      },
      "outputs": [],
      "source": [
        "def detect_objects(image):\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "  sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "  sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "  sobel_combined = np.uint8(sobel_combined)\n",
        "\n",
        "  _, binary_image = cv2.threshold(sobel_combined, np.mean(sobel_combined) * 2, 255, cv2.THRESH_BINARY)\n",
        "  imshow(binary_image)\n",
        "\n",
        "  contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  contour_image = img1.copy()\n",
        "\n",
        "  valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 1500]\n",
        "\n",
        "  detected_objects = []\n",
        "\n",
        "  for contour in valid_contours:\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "      cropped_image = image[y:y + h, x:x + w]\n",
        "      detected_objects.append(cropped_image)\n",
        "\n",
        "      imshow( cropped_image)\n",
        "\n",
        "  return detected_objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w7pOQVIYn9C-",
        "outputId": "cafdbc8a-df92-467a-d28d-b496d1287607"
      },
      "outputs": [],
      "source": [
        "detected_objects = detect_objects(img1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25JlTWWKmp9s"
      },
      "source": [
        "### Idea is to create templates for matching different objects, so far:\n",
        " - Capitol\n",
        " - Any non reversed card (template for a card is just a random card)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ssZEKigGVs_Z",
        "outputId": "32123b7b-865a-4432-c3d3-ee3e2317d78d"
      },
      "outputs": [],
      "source": [
        "template1 = cv2.imread('template1.png')\n",
        "template2 = cv2.imread('template2.png')\n",
        "templates = [template1, template2]\n",
        "for template in templates:\n",
        "  imshow(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ackEj2s4m5wc"
      },
      "source": [
        "## Matching templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuXaB19m-9f"
      },
      "source": [
        "Templates are scaled to size of detected object and then each template is matched to each object to return which objects match given template, instead of hard coded value we would like to explore determining objects by most simmilar template given some small threshold is passed to exclude noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZVRy11yZHSng"
      },
      "outputs": [],
      "source": [
        "def match_templates(templates, detected_objects):\n",
        "  matches = []\n",
        "  for template in templates:\n",
        "    print(f'Matching template:')\n",
        "    imshow(template)\n",
        "    print()\n",
        "    for i, obj in enumerate(detected_objects):\n",
        "      template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      template_resized = cv2.resize(template_gray, (obj.shape[:2][::-1]))\n",
        "      obj_gray = cv2.cvtColor(obj, cv2.COLOR_BGR2GRAY)\n",
        "      result = cv2.matchTemplate(obj_gray, template_resized, cv2.TM_CCOEFF_NORMED)\n",
        "      min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "\n",
        "      matches.append(max_val)\n",
        "      threshold = 0.3\n",
        "      if max_val > threshold:\n",
        "          # This contour matches template1\n",
        "          print(f\"Match! {i}\")\n",
        "          imshow(obj)\n",
        "    print('-'*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdYdJcGilCuf",
        "outputId": "18ffe9e8-bf62-4390-f3ac-2e2eafbd8d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matching template:\n",
            "\n",
            "Match! 1\n",
            "Match! 2\n",
            "Match! 3\n",
            "----------\n",
            "Matching template:\n",
            "\n",
            "Match! 4\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "match_templates(templates, detected_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fApuE3Unf4E"
      },
      "source": [
        "## Problems:\n",
        " - there are 6 fractions each having cards in different color\n",
        " - there will probably be need to preprocess images to take care of lighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shRH7t_tofFX"
      },
      "source": [
        "We thought about 2 ways of dealing with first problem:\n",
        "1. Based on results below it could be possible to create generic templates(gray) that could match each fraction and if possible exclude art from template so it wouldn't get compared\n",
        "\n",
        "2. Create 6 sets of templates for each faction. Pretty self explanatory 6 is not a big number this shouldn't increase computational need greatly so maybe this approach would yield better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6X3WyCz1nxU8",
        "outputId": "a5e8e62f-1b5b-442e-c9c1-b61a49a2f579"
      },
      "outputs": [],
      "source": [
        "detected_objects = detect_objects(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p8xudRYjoQf-",
        "outputId": "4fa3dcef-cc3a-4470-aaa2-0f9787819efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matching template:\n",
            "\n",
            "Match! 2\n",
            "Match! 4\n",
            "----------\n",
            "Matching template:\n",
            "\n",
            "Match! 3\n"
          ]
        }
      ],
      "source": [
        "match_templates(templates, detected_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WodwCVpQ8u"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4OIbIqIpT40"
      },
      "source": [
        "We believe we could make this method work pretty well on photos, however we are not sure how it generalizes to a video, making computations each frame probbably is quite intense, so we could try detecting objects at start of a video and the classify any new objects that were moved\n",
        "\n",
        "We would greatly appreciate feedback and any form of guidence to a reasonable approach"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
