{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgQNzc5O1MQh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFQd2X0M1Uc7"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    imshow = cv2_imshow\n",
        "else:\n",
        "\n",
        "    def imshow(img):\n",
        "        cv2.imshow(\"ImageWindow\", img)\n",
        "        cv2.waitKey()\n",
        "\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvbuM52Mqqiy"
      },
      "outputs": [],
      "source": [
        "img1  = cv2.imread('resources/img1.jpg')\n",
        "img2  = cv2.imread('resources/img4.jpg')\n",
        "img1 = cv2.resize(img1, None, fx=0.2, fy=0.2)\n",
        "img2 = cv2.resize(img2, None, fx=0.2, fy=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy8AJuTV2Kx6"
      },
      "outputs": [],
      "source": [
        "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntzhvoMbmiKk"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yzymTLaqqJJ"
      },
      "outputs": [],
      "source": [
        "def perspective_transform(image, corners):\n",
        "    # ordered_corners = order_corner_points(corners)\n",
        "    ordered_corners = corners\n",
        "    bottom_l, bottom_r, top_r, top_l = ordered_corners\n",
        "\n",
        "    width_A = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n",
        "    width_B = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n",
        "    width = max(int(width_A), int(width_B))\n",
        "\n",
        "    height_A = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n",
        "    height_B = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n",
        "    height = max(int(height_A), int(height_B))\n",
        "\n",
        "    dimensions = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1],\n",
        "                    [0, height - 1]], dtype = \"float32\")\n",
        "\n",
        "    ordered_corners = np.array(ordered_corners, dtype=\"float32\")\n",
        "\n",
        "    matrix = cv2.getPerspectiveTransform(ordered_corners, dimensions)\n",
        "\n",
        "    return cv2.warpPerspective(image, matrix, (width, height))\n",
        "\n",
        "def order_corner_points(corners):\n",
        "    corners = [(corner[0][0], corner[0][1]) for corner in corners]\n",
        "    top_r, top_l, bottom_l, bottom_r = corners[0], corners[1], corners[2], corners[3]\n",
        "    return (top_l, top_r, bottom_r, bottom_l)\n",
        "\n",
        "def find_extreme_points(contour):\n",
        "    rect = cv2.minAreaRect(contour)\n",
        "    box = cv2.boxPoints(rect)\n",
        "    box = np.int0(box)\n",
        "\n",
        "    bottom_left = tuple(box[0])\n",
        "    top_left = tuple(box[3])\n",
        "    bottom_right = tuple(box[1])\n",
        "    top_right = tuple(box[2])\n",
        "\n",
        "    return bottom_left, bottom_right, top_right, top_left\n",
        "\n",
        "def detect_corners(image):\n",
        "    image = image.copy()\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    blurred_image = cv2.blur(gray, kernel_size)\n",
        "\n",
        "    sobel_x = cv2.Sobel(blurred_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(blurred_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "    magnitude = np.uint8(magnitude)\n",
        "\n",
        "    _, thresh = cv2.threshold(magnitude, 30, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    corners = find_extreme_points(largest_contour)\n",
        "\n",
        "    return np.array(corners)\n",
        "\n",
        "    # corner_points = []\n",
        "\n",
        "    # epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "    # approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "    # if len(approx) == 4:\n",
        "    #     corner_points.extend(approx)\n",
        "\n",
        "    # corner_points = np.array(corner_points)\n",
        "    # return corner_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntzhvoMbmiKk"
      },
      "source": [
        "## Detecting objects on a photo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-_8QizEJuLH"
      },
      "outputs": [],
      "source": [
        "def detect_objects(image):\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "  sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "  sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "  sobel_combined = np.uint8(sobel_combined)\n",
        "\n",
        "  _, binary_image = cv2.threshold(sobel_combined, np.mean(sobel_combined) * 2, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "  contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 4_000]\n",
        "\n",
        "  detected_objects = []\n",
        "  detected_cords = []\n",
        "  for contour in valid_contours:\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "      detected_cords.append((x, y, w, h))\n",
        "      cropped_image = image[y:y + h, x:x + w]\n",
        "      detected_objects.append(cropped_image)\n",
        "\n",
        "  return detected_objects, detected_cords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7pOQVIYn9C-"
      },
      "outputs": [],
      "source": [
        "detected_objects, detected_cords = detect_objects(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25JlTWWKmp9s"
      },
      "outputs": [],
      "source": [
        "for i, obj in enumerate(detected_objects):\n",
        "  corners = detect_corners(obj)\n",
        "  if len(corners > 0):\n",
        "    transformed_image = perspective_transform(obj, corners)\n",
        "    detected_objects[i] = transformed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25JlTWWKmp9s"
      },
      "source": [
        "### Idea is to create templates for matching different objects, so far:\n",
        " - Capitol\n",
        " - Any non reversed card (template for a card is just a random card)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBknuH-PE9CV"
      },
      "outputs": [],
      "source": [
        "# corners = detect_corners(template4)\n",
        "# print(len(corners))\n",
        "# transformed = perspective_transform(template4, corners)\n",
        "# imshow(transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ackEj2s4m5wc"
      },
      "outputs": [],
      "source": [
        "classes = ['Capitol', 'Unit', 'Support', 'Deck', 'Opponent_capitol', 'Opponent_deck', 'Opponent_support']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buuXaB19m-9f"
      },
      "outputs": [],
      "source": [
        "template1 = cv2.imread('resources/template3.png')\n",
        "template2 = cv2.imread('resources/template4.png')\n",
        "template3 = cv2.imread('resources/template6.png')\n",
        "template4 = cv2.imread('resources/deck_template.png')\n",
        "template5 = cv2.imread('resources/reverse_deck.png')\n",
        "template6 = cv2.imread('resources/template1.png')\n",
        "Opp_capitol = cv2.imread('resources/template2.png')\n",
        "templates = [template1, template2, template3, template4, template5, template6]\n",
        "# for template in templates:\n",
        "#   imshow(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Used templates:\n",
        "![Capitol template](resources/template3.png)\n",
        "![Unit template](resources/template4.png)\n",
        "![Support template](resources/template6.png)\n",
        "![Deck template](resources/deck_template.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ackEj2s4m5wc"
      },
      "source": [
        "## Matching templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuXaB19m-9f"
      },
      "source": [
        "Templates are scaled to size of detected object and then each template is matched to each object to return which objects match given template, instead of hard coded value we would like to explore determining objects by most simmilar template given some small threshold is passed to exclude noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVRy11yZHSng"
      },
      "outputs": [],
      "source": [
        "def match_templates(templates, detected_objects):\n",
        "  matches = []\n",
        "  determined_classes = [[] for _ in range(len(detected_objects))]\n",
        "  for template in templates:\n",
        "    for i, obj in enumerate(detected_objects):\n",
        "      template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      template_resized = cv2.resize(template_gray, (obj.shape[:2][::-1]))\n",
        "      obj_gray = cv2.cvtColor(obj, cv2.COLOR_BGR2GRAY)\n",
        "      result = cv2.matchTemplate(obj_gray, template_resized, cv2.TM_CCOEFF_NORMED)\n",
        "      min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "      determined_classes[i].append(max_val)\n",
        "      matches.append(max_val)\n",
        "\n",
        "  return determined_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "uF9qqKFiqaqU",
        "outputId": "7073ef1b-dcb8-4e11-9957-73421a53d5a7"
      },
      "outputs": [],
      "source": [
        "for obj in detected_objects:\n",
        "  imshow(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdYdJcGilCuf"
      },
      "outputs": [],
      "source": [
        "determined_classes = match_templates(templates, detected_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH4jrebu5vQ7"
      },
      "outputs": [],
      "source": [
        "def draw_detections(image, detected_cords, determined_classes, classes):\n",
        "  original_image = image.copy()\n",
        "  for i, obj in enumerate(detected_cords):\n",
        "      x, y, width, height = obj\n",
        "      class_num = np.argmax(determined_classes[i])\n",
        "      obj_class = classes[np.argmax(determined_classes[i])]\n",
        "\n",
        "      colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (120, 120, 120)]  # Example colors for three classes\n",
        "      color = colors[class_num]\n",
        "\n",
        "      # Add text label\n",
        "      if determined_classes[i][class_num] > 0.2:\n",
        "        label = obj_class\n",
        "      else:\n",
        "        label = 'Not classified'\n",
        "        color = colors[-1]\n",
        "\n",
        "      # Draw the rectangle\n",
        "      cv2.rectangle(original_image, (x, y), (x + width, y + height), color, 2)\n",
        "      cv2.putText(original_image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  return original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fApuE3Unf4E"
      },
      "outputs": [],
      "source": [
        "imshow(draw_detections(img2, detected_cords, determined_classes, classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classes[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shRH7t_tofFX"
      },
      "outputs": [],
      "source": [
        "video_path = 'resources/vid3.mp4'  # Replace with the path to your video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Couldn't open video.\")\n",
        "    exit()\n",
        "\n",
        "# Get video information\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Define the codec and create a video writer object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can change the codec as needed\n",
        "output_video_path = 'outputs/video.avi'  # Replace with the desired output video file path\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "frame_number = 0\n",
        "process_every_n_frames = 20  # Process every 5th frame\n",
        "deck_detected = False\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # Perform object detection on the current frame every 5 frames\n",
        "    if frame_number % process_every_n_frames == 0:\n",
        "        detected_objects, detected_cords = detect_objects(frame)\n",
        "        determined_classes = match_templates(templates, detected_objects)\n",
        "        detected_this_frame = False\n",
        "        for obj_class in determined_classes:\n",
        "          if np.argmax(obj_class) == 3 and obj_class[np.argmax(obj_class)] > 0.2:\n",
        "            deck_detected = True\n",
        "            detected_this_frame = True\n",
        "            break\n",
        "        if not detected_this_frame:\n",
        "          deck_detected = False\n",
        "          print(1)\n",
        "    if not deck_detected:\n",
        "       cv2.putText(frame, \"Card Drawn!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "    # Draw rectangles and labels on the frame\n",
        "    frame = draw_detections(frame, detected_cords, determined_classes, classes)\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
        "        break\n",
        "\n",
        "print(frame_number)\n",
        "\n",
        "# Release the video capture and writer objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all OpenCV windows\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fApuE3Unf4E"
      },
      "source": [
        "## Problems:\n",
        " - there are 6 fractions each having cards in different color\n",
        " - there will probably be need to preprocess images to take care of lighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shRH7t_tofFX"
      },
      "source": [
        "We thought about 2 ways of dealing with first problem:\n",
        "1. Based on results below it could be possible to create generic templates(gray) that could match each fraction and if possible exclude art from template so it wouldn't get compared\n",
        "\n",
        "2. Create 6 sets of templates for each faction. Pretty self explanatory 6 is not a big number this shouldn't increase computational need greatly so maybe this approach would yield better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6WodwCVpQ8u"
      },
      "outputs": [],
      "source": [
        "detected_objects = detect_objects(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4OIbIqIpT40"
      },
      "outputs": [],
      "source": [
        "match_templates(templates, detected_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WodwCVpQ8u"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4OIbIqIpT40"
      },
      "source": [
        "We believe we could make this method work pretty well on photos, however we are not sure how it generalizes to a video, making computations each frame probbably is quite intense, so we could try detecting objects at start of a video and the classify any new objects that were moved\n",
        "\n",
        "We would greatly appreciate feedback and any form of guidence to a reasonable approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqTV4OXOae0v"
      },
      "outputs": [],
      "source": [
        "gray = cv2.cvtColor(template2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "kernel_size = (7, 7)\n",
        "\n",
        "blurred_image = cv2.blur(gray, kernel_size)\n",
        "\n",
        "sobel_x = cv2.Sobel(blurred_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "sobel_y = cv2.Sobel(blurred_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "sobel_combined = np.uint8(sobel_combined)\n",
        "\n",
        "imshow(sobel_combined)\n",
        "\n",
        "contours, _ = cv2.findContours(sobel_combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "rect = cv2.minAreaRect(approx)\n",
        "box = cv2.boxPoints(rect)\n",
        "box = np.int0(box)\n",
        "\n",
        "width, height = 200, 300\n",
        "dst_pts = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=\"float32\")\n",
        "matrix = cv2.getPerspectiveTransform(box.astype(\"float32\"), dst_pts)\n",
        "\n",
        "rectified_card = cv2.warpPerspective(template2, matrix, (width, height))\n",
        "\n",
        "imshow(template2)\n",
        "imshow(rectified_card)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTeDnYZ7ifjG"
      },
      "outputs": [],
      "source": [
        "def order_corner_points(corners):\n",
        "    # Separate corners into individual points\n",
        "    # Index 0 - top-right\n",
        "    #       1 - top-left\n",
        "    #       2 - bottom-left\n",
        "    #       3 - bottom-right\n",
        "    corners = [(corner[0][0], corner[0][1]) for corner in corners]\n",
        "    top_r, top_l, bottom_l, bottom_r = corners[0], corners[1], corners[2], corners[3]\n",
        "    return (top_l, top_r, bottom_r, bottom_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK4ld8Mzd0Ut"
      },
      "outputs": [],
      "source": [
        "def perspective_transform(image, corners):\n",
        "    # Order points in clockwise order\n",
        "    ordered_corners = order_corner_points(corners)\n",
        "    top_l, top_r, bottom_r, bottom_l = ordered_corners\n",
        "\n",
        "    # Determine width of new image which is the max distance between\n",
        "    # (bottom right and bottom left) or (top right and top left) x-coordinates\n",
        "    width_A = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n",
        "    width_B = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n",
        "    width = max(int(width_A), int(width_B))\n",
        "\n",
        "    # Determine height of new image which is the max distance between\n",
        "    # (top right and bottom right) or (top left and bottom left) y-coordinates\n",
        "    height_A = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n",
        "    height_B = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n",
        "    height = max(int(height_A), int(height_B))\n",
        "\n",
        "    # Construct new points to obtain top-down view of image in\n",
        "    # top_r, top_l, bottom_l, bottom_r order\n",
        "    dimensions = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1],\n",
        "                    [0, height - 1]], dtype = \"float32\")\n",
        "\n",
        "    # Convert to Numpy format\n",
        "    ordered_corners = np.array(ordered_corners, dtype=\"float32\")\n",
        "\n",
        "    # Find perspective transform matrix\n",
        "    matrix = cv2.getPerspectiveTransform(ordered_corners, dimensions)\n",
        "\n",
        "    # Return the transformed image\n",
        "    return cv2.warpPerspective(image, matrix, (width, height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM00ZccAd5T4"
      },
      "outputs": [],
      "source": [
        "image = template2.copy()\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "kernel_size = (7, 7)\n",
        "\n",
        "blurred_image = cv2.blur(gray, kernel_size)\n",
        "\n",
        "sobel_x = cv2.Sobel(blurred_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "sobel_y = cv2.Sobel(blurred_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "sobel_combined = np.uint8(sobel_combined)\n",
        "\n",
        "# Apply thresholding to obtain a binary image\n",
        "_, thresh = cv2.threshold(sobel_combined, 50, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Find contours in the binary image\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "largest_contour = max(contours, key=cv2.contourArea)\n",
        "cv2.drawContours()\n",
        "imshow(sobel_combined)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
